# Lezione 25
## Applicazioni Lineari
##### Teorema di Struttura
Sia $v^{0}\in\mathbb{R}^{n}$ una soluzione del sistema lineare $Ax=b$ di ordine $n.$ allora ogni altra soluzione è della forma $v=v^{0}+w,$ dove $w\in\mathbb{R}^{n}$ è una soluzione del sistema omogeneo $Ax=O.$ In altre parole, se $L\subseteq \mathbb{R}^{n}$ è l'insieme delle soluzioni del sistema $Ax=b$ e $W\subseteq\mathbb{R}^{n}$ è l'insieme delle soluzioni del sistema $Ax=b$ e $W\subseteq\mathbb{R}^{n}$ e l'insieme delle soluzioni del sistema omogeneo associato $Ax=O,$ si ha $$L=v^{0}+W=\{v^{0}+w|w\in W \}.$$In particolare $v^{0}$ è l'unica soluzione del sistema se e solo se le colonne di $A$ sono linearmente indipendenti.
###### Dimostrazione 
Posto $v^0=(v^0_{1},...v^0_{n}),$ sia $w=(w_{1},...,w_{n})$ una soluzione del sistema omogeneo. Allora: $$v^{0}_{1}A^{1}+...+v^{0}_{n}A^{n}=b\quad\text{e}\quad w_{1}A^{1}+...+w_{n}A^{n}=O,$$per cui sommando otteniamo $$(v_{1}^{0}+w_{1})A^{1}+...+(v^{0}_{n}+w_{n})A^{n}=b$$cioè $v^{0}+w$ è un'altra soluzione del sistema $Ax=b.$

Sia ora $v$ un'altra soluzione di $Ax=b;$ dobbiamo dimostrare che $w=v-v^{0}$ è una soluzione del sistema $Ax=O.$ Infatti abbiamo $$v_{1}A^{1}+...+v_{n}A^{n}=b\quad\text{e}\quad v^{0}_{1}A_{1}+...+v^{n}_{1}A^{n}=b;$$sottraendo otteniamo $(v_{1}-v^{0}_{1})A^{1}+...+(v_{n}-v^{0}_{n})A^{n}=O,$ come voluto. 
#### Applicazioni Lineari
Sia $a\in M_{m,n}(\mathbb{R})$ una matrice con $m$ righe e $n$ colonne, cioè $$A=\begin{vmatrix}a_{11}&\dots&a_{1n}\\ \vdots & \ddots & \vdots\\ a_{m1} & \dots & a_{mn} \end{vmatrix}=\begin{vmatrix}A^{1}...A^{n} \end{vmatrix},$$con $A^{1},...A^{n}\in\mathbb{R}^{m}.$ Allora definiamo un'applicazione $L_{A}:\mathbb{R}^{n}\longrightarrow\mathbb{R}^{m}$ ponendo $$L_{A}(x)=\begin{vmatrix}a_{11}x_{1}&\dots&a_{1n}x_{n}\\ \vdots & \ddots & \vdots\\ a_{m1}x_{1} & \dots & a_{mn}x_{n} \end{vmatrix}=x_{1}A^{1}+...+x_{n}A^{n}\in\mathbb{R}^{m},$$per ogni $x\in \mathbb{R}^{m}.$

Notiamo che $x$ è soluzione del sistema $Ax=b$ se e solo se $L_{A}(x)=b.$ 
$L_{A}(x)$ si può scrivere anche come $Ax$ o $A\begin{vmatrix}x_{1}\\ \vdots \\ x_{n}\end{vmatrix}.$
##### Proprietà di $L_{A}$
Affinché $L_{A}(x)$ abbia senso, occorre che il numero di colonne di $A$ coincida con il numero di coordinate (di righe) del vettore $x.$ Utilizzando una matrice $M_{m,n}(\mathbb{K})$ a coefficienti in $\mathbb{K}$ possiamo definire del tutto in maniera analoga applicazione $L_{A}:\mathbb{K}^{n}\longrightarrow\mathbb{K}^{m}.$ 

Le applicazioni $L_{A}$ hanno due proprietà fondamentali:$$L_{A}(x+y)=L_{A}(x)+L_{A}(y)\quad  (\star)\quad\text{e}\quad L_{A}(\lambda x)=\lambda L_{A}(x)\quad (\star \star )$$
###### Dimostrazione
- $(\star):$
	Siano $$\begin{align} & L_{A}(x)=\begin{vmatrix}a_{11}x_{1}&\dots&a_{1n}x_{n}\\ \vdots & \ddots & \vdots\\ a_{m1}x_{1} & \dots & a_{mn}x_{n} \end{vmatrix}=x_{1}A^{1}+...+x_{n}A^{n}\in\mathbb{R}^{m}, \\
    & L_{A}(y)=\begin{vmatrix}a_{11}y_{1}&\dots&a_{1n}y_{n}\\ \vdots & \ddots & \vdots\\ a_{m1}y_{1} & \dots & a_{mn}y_{n} \end{vmatrix}=y_{1}A^{1}+...+y_{n}A^{n}\in\mathbb{R}^{m}, \end{align}$$ Allora: $$\begin{align} & L_{A}(x)+L_{A}(y)=  (x)=\begin{vmatrix}a_{11}x_{1}+a_{11}y_{1}&\dots&a_{1n}x_{n}+a_{1n}y_{n}\\ \vdots & \ddots & \vdots\\ a_{m1}x_{1}+a_{m1}y_{1} & \dots & a_{mn}x_{n}+a_{mn}y_{n} \end{vmatrix}= \\
    & \implies \begin{vmatrix}a_{11}(x_{1}+y_{1})&\dots&a_{1n}(x_{n}+y_{n})\\ \vdots & \ddots & \vdots\\ a_{m1}(x_{1}+y_{1}) & \dots & a_{mn}(x_{n}+y_{n}) \end{vmatrix}=L_{A}(x+y)\end{align}$$
 - $(\star\star):$
	 Siano $$\begin{align} & L_{A}(x)=\begin{vmatrix}a_{11}x_{1}&\dots&a_{1n}x_{n}\\ \vdots & \ddots & \vdots\\ a_{m1}x_{1} & \dots & a_{mn}x_{n} \end{vmatrix}=x_{1}A^{1}+...+x_{n}A^{n}\in\mathbb{R}^{m} \end{align},\quad\lambda\in \mathbb{R}$$ Allora:$$\begin{align} & L_{A}(\lambda x)=\begin{vmatrix}\lambda a_{11}x_{1}&\dots&\lambda a_{1n}x_{n}\\ \vdots & \ddots & \vdots\\ \lambda a_{m1}x_{1} & \dots & \lambda a_{mn}x_{n} \end{vmatrix}=\lambda x_{1}A^{1}+...+\lambda x_{n}A^{n}\in\mathbb{R}^{m} \\
     & =\lambda\begin{vmatrix}a_{11}x_{1}&\dots&a_{1n}x_{n}\\ \vdots & \ddots & \vdots\\ a_{m1}x_{1} & \dots & a_{mn}x_{n} \end{vmatrix}=\lambda\cdot  L_{A}(x)\end{align}$$
###### Definizione - Applicazione Lineare
Un'**applicazione lineare** (o trasformazione lineare) fra due spazi vettoriali $V$ e $W$ è una funzione $T:V\longrightarrow W$ tale che:
1) $T(v_{1}+v_{2})=T(v_{1})+T(v_{2})$ per tutti i  $v_{1},v_{2}\in V$ (diremo che $T$ è additiva).
2) $T(\lambda v)=\lambda T(v)$ per tutti i $\lambda\in\mathbb{R}$ e $v\in V$ (diremo che $T$ è omogenea).
###### Esempio - Applicazione Lineare
Sia $T:\mathbb{R}^{3}\rightarrow\mathbb{R}^{3}$ data da $$T\begin{vmatrix} x \\  y \\ z\end{vmatrix} = \begin{vmatrix} 3x-2y \\ z \\ x+z\end{vmatrix}.$$
Allora abbiamo: $$\begin{align}& T\left(\begin{vmatrix}x_{1}\\y_{1}\\z_{1} \end{vmatrix}+\begin{vmatrix}x_{2}\\y_{2}\\z_{2}  \end{vmatrix}\right)=T\begin{vmatrix}x_{1}+x_{2}\\y_{1}+y_{2}\\z_{1}+z_{2} \end{vmatrix}= \begin{vmatrix} 3(x_{1}+x_{2})-2(y_{1}+y_{2})\\z_{1}+z_{2}\\(x_{1}+x_{2})+(z_{1}+z_{2} )\end{vmatrix} \\
& =\begin{vmatrix}3x_{1}-2y_{1}\\z_{1}\\x_{1}+z_{1} \end{vmatrix}+\begin{vmatrix}3x_{2}-2y_{2}\\z_{2}\\x_{2}+z_{2} \end{vmatrix}= T\begin{vmatrix}x_{1}\\y_{1}\\z_{1} \end{vmatrix} + T\begin{vmatrix} x_{2}\\y_{2}\\z_{2}\end{vmatrix}\end{align}$$e$$T\left(\lambda\begin{vmatrix}x\\y\\z \end{vmatrix}\right)=T\begin{vmatrix}\lambda x\\\lambda y\\\lambda z\end{vmatrix}=\begin{vmatrix}3\lambda x-2\lambda y\\\lambda z\\\lambda x+\lambda z \end{vmatrix}=\lambda\begin{vmatrix} 3x-2y\\z\\x+z \end{vmatrix}=\lambda T\begin{vmatrix}x\\y\\z \end{vmatrix}$$ per cui $T$ è lineare. Notiamo che $T=L_{A},$ dove $A=\begin{vmatrix}3&-2&0\\0&0&1\\1&0&1\end{vmatrix}.$
###### Proposizione - Unicità delle applicazioni lineari
Siano $V$ e $W$ due spazi vettoriali, $\{v_{1},...,v_{n} \}$ una base di $V,$ e $w_{1},...,w_{n}\in W.$ Allora esiste un'unica applicazione lineare $T:V\longrightarrow W$ tale che $T(v_{j})=w_{j}$ per $j=1,...n.$ L'applicazione $T$ è tale che $$T(\alpha_{1}v_{1}+...+\alpha_{n}v_{n})=\alpha_{1}w_{1}+...+\alpha_{n}w_{n}$$per ogni $\alpha_{1},...,\alpha_{n}\in\mathbb{K}.$

Dimostrazione:
1) Dimostrazione dell'esistenza.
Presi $v,v'\in V,$ scriviamoli come combinazione lineare dei vettori della base di V:
$$v=\alpha_{1}v_{1}+...+\alpha_{n}v_{n},\quad\text{e}\quad v'=\alpha'_{1}v_{1}+...+\alpha'_{n}v_{n}.$$Allora $v+v'=(\alpha_{1}+\alpha'_{1})v_{1}+...+(\alpha_{n}+\alpha'_{n})v_{n}$ e $$\begin{align}&T(v+v')=(\alpha_{1}+\alpha'_{1})w_{1}+...+(\alpha_{1}+\alpha'_{1})w_{n}\\&=(\alpha_{1}w_{1}+...+\alpha_{n}w_{n})+(\alpha'_{1}w_{1}+...+\alpha'_{n}w_{n})\\&=T(v)+T(v'). \end{align}$$
Nello stesso modo si dimostra che$T(\lambda v)=\lambda T(v).$ 
2) Dimostrazione dell'unicità.
Sia $S:V\longrightarrow W$ un'altra applicazione lineare tale che $S(v_{j})=w_{j}$ per $j=1,...,n;$ dobbiamo dimostrare che $S(v)=T(v)$ qualunque sia $v\in V.$ 
Prendiamo allora $v\in V$ e lo scriviamo come combinazione lineare degli elementi della base, $v=\alpha_{1}v_{1}+...+\alpha_{n}v_{n}.$ Allora: $$\begin{align}&S(v)=S(\alpha_{1}v_{1}+...+\alpha_{n}v_{n})=\alpha_{1}S(v_{1})+...+\alpha_{n}S(v_{n})=\\& = \alpha_{1}w_{1}+...+\alpha_{n}w_{n}=T(v). \end{align}$$
###### Corollario - Applicazioni lineari coincidenti
Siano $V$ e $W$ due spazi vettoriali, $\{v_{1},...,v_{n} \},$ una base di $V,$ e $S,$ $T:V\longrightarrow W$ due applicazioni lineari. Supponiamo che si abbia $S(v_{j})=T(v_{j})$ per $j=1,...,n.$ Allora $S\equiv T,$ cioè $S(v) = T(v)$ per tutti i vettori $v\in V.$

Dimostrazione
Dall'unicità che abbiamo dimostrato prima con $w_{j}=T(v_{j})$ per $j=1,...,n.$ due matrici diverse danno sempre origine ad applicazioni lineari diverse.
###### Proposizione - Identità delle trasformazioni implica identità delle matrici
Siano $A,B\in M_{m,n}(\mathbb{K})$ due matrici. Allora $L_{A}\equiv L_B$ se e solo se $A=B.$
Dimostrazione:
Chiaramente se $A=B$ allora $L_{A}\equiv L_{B}.$ Viceversa, supponiamo che $L_{A}\equiv L_{B}.$ Siccome si ha $\forall j=1,...,n$ e $L_{A}(e_{j})=A^{j}$ (dove $e_{1},...,e_{j}$ è la base canonica di $\mathbb{K}^n$), le due matrici $A$ e $B$ devono avere le stesse colonne, per cui sono uguali.
#### Nucleo e Immagine
A ogni applicazione lineare $T:V\longrightarrow W$ possiamo associare due sottoinsiemi:
1) Il **nucleo** $Ker(T)=\{v\in V|T(v)=O \}\subseteq V;$
2) L'**immagine** $Im(T)=T(V)=\{T(v)|v\in V \}\subseteq W.$
Il fatto interessante è che nucleo e immagine, oltre a essere sottospazi vettoriali, caratterizzano l'iniettività e la suriettività delle applicazioni lineari.
###### Proposizione - Iniettività e Suriettività dipendenti da Nucleo e Immaigne
Sia $T: V\longrightarrow W$ un'applicazione lineare. Allora:
1) $Ker(T)$ è un sottospazio di $V;$
2) $Im(T)$ è un sottospazio di $W;$
3) $T$ è suriettiva se e solo se $Im(T) =W;$
4) $T$ è iniettiva se e solo se $Ker(T) = \{O \}.$

Dimostrazione:
1) Dimostriamo che il nucleo è chiuso rispetto alla somma; Prendiamo $v_{1},v_{2}\in Ker(T),$ cioè tali che si abbia $T(v_{1})=T(v_{2})=O;$ dobbiamo dimostrare che $v_{1}+v_{2}\in Ker(T).$ Notiamo che $T(v_{1}+v_{2})=T(v_{1})+T(v_{2}) =O+O=O.$
3) Stavolta dimostriamo che l'immagine è chiusa rispetto al prodotto per scalari. Siano $\lambda\in\mathbb{K}$ e $T(v)\in Im(T).$ Dobbiamo dimostrare che $T(\lambda v)\in Im(T).$ Semplicemente, si ha che $\lambda T(v)= T(\lambda v)$ per le proprietà dell'applicazione lineare, da cui segue $T(\lambda v)\in Im(T).$
4) Ricordando la definizione di immagine dell'applicazione e della suriettività, se ogni elemento di $W$ è immagine tramite l'applicazione di almeno un elemento di $V$, allora $Im(T)=W;$
5) Se $T$ è iniettiva $T(v)=O=T(O)$ implica $v=O$ e quindi $KerT=\{O \}.$ Viceversa, supponiamo $KerT=\{O \}.$ Prendiamo $v_{1},v_{2}\in V$ tali che $T(v_{1})=T(v_{2});$ dobbiamo dimostrare che $v_{1} = v_{2}:$ $$\begin{align}& T(v_{1})=T(v_{2})\longrightarrow T(v_{1})-T(v_{2})=O\\ &\longrightarrow T(v_{1}-v_{2})=O \longrightarrow v_{1}-v_{2}=O\\& \longrightarrow v_{1}=v_{2}\end{align}$$ Dove la penultima implicazione segue dall'ipotesi che $KerT=\{O\}.$


